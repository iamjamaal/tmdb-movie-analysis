FROM bitnami/spark:3.5.0

USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3-pip \
    curl \
    vim \
    git \
    wget \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Create directories
RUN mkdir -p /opt/spark-apps /opt/spark-data /opt/logs \
    && chmod -R 777 /opt/spark-apps /opt/spark-data /opt/logs

# Install Python dependencies
COPY requirements.txt /tmp/requirements.txt
RUN pip3 install --no-cache-dir -r /tmp/requirements.txt

# Copy application code
COPY src/ /opt/spark-apps/src/

# Set environment variables
ENV PYTHONPATH=/opt/spark-apps:$PYTHONPATH
ENV SPARK_HOME=/opt/bitnami/spark
ENV PATH=$SPARK_HOME/bin:$PATH

# Configure Spark
RUN echo "spark.driver.extraJavaOptions -Dlog4j.configuration=file:///opt/bitnami/spark/conf/log4j.properties" >> $SPARK_HOME/conf/spark-defaults.conf && \
    echo "spark.executor.extraJavaOptions -Dlog4j.configuration=file:///opt/bitnami/spark/conf/log4j.properties" >> $SPARK_HOME/conf/spark-defaults.conf

# Set working directory
WORKDIR /opt/spark-apps

# Switch back to spark user
USER 1001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8080/ || exit 1

# Default command (overridden by docker-compose)
CMD ["/opt/bitnami/scripts/spark/run.sh"]
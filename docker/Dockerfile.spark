FROM apache/spark:3.5.0

USER root

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3-pip \
    curl \
    vim \
    git \
    wget \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Create directories
RUN mkdir -p /opt/spark-apps /opt/spark-data /opt/logs \
    && chmod -R 777 /opt/spark-apps /opt/spark-data /opt/logs

# Install Python dependencies
COPY requirements-spark.txt /tmp/requirements.txt
RUN pip3 install --no-cache-dir -r /tmp/requirements.txt

# Copy application code
COPY src/ /opt/spark-apps/src/

# Copy entrypoint script
COPY docker/spark-entrypoint.sh /usr/local/bin/spark-entrypoint.sh
RUN chmod +x /usr/local/bin/spark-entrypoint.sh

# Set environment variables
ENV PYTHONPATH=/opt/spark-apps:$PYTHONPATH
ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH

# Create Spark config directory if it doesn't exist and configure Spark
RUN mkdir -p $SPARK_HOME/conf && \
    touch $SPARK_HOME/conf/spark-defaults.conf && \
    echo "spark.driver.extraJavaOptions -Dlog4j.configuration=file://$SPARK_HOME/conf/log4j.properties" >> $SPARK_HOME/conf/spark-defaults.conf && \
    echo "spark.executor.extraJavaOptions -Dlog4j.configuration=file://$SPARK_HOME/conf/log4j.properties" >> $SPARK_HOME/conf/spark-defaults.conf

# Set working directory
WORKDIR /opt/spark-apps

# Switch back to spark user
USER 185

# Use entrypoint script
ENTRYPOINT ["/usr/local/bin/spark-entrypoint.sh"]
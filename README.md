# ğŸ¬ TMDB Movie Data Analysis Pipeline

A production-grade, scalable data engineering pipeline for analyzing movie data from The Movie Database (TMDB) API using Apache Spark, Airflow, and modern data engineering practices.

## ğŸ“Š Project Overview

This project transforms raw movie data from TMDB API into actionable insights through:
- **Distributed Data Processing** with Apache Spark
- **Workflow Orchestration** with Apache Airflow
- **Intelligent Caching** with Redis
- **Data Quality Validation** with custom validators
- **Advanced Analytics** with comprehensive KPIs
- **Interactive Visualizations** with Matplotlib/Seaborn

## ğŸŒŸ Key Features

### Data Engineering
âœ… **Scalable Architecture**: Spark cluster for distributed processing  
âœ… **Automated Workflows**: Airflow DAGs for orchestration  
âœ… **Smart Caching**: Redis-based caching to minimize API calls  
âœ… **Rate Limiting**: Token bucket algorithm for API protection  
âœ… **Error Handling**: Comprehensive retry logic and fallback mechanisms  

### Data Quality
âœ… **Schema Validation**: Automated schema checking  
âœ… **Business Rule Validation**: Custom validation rules  
âœ… **Data Completeness Checks**: Missing value detection  
âœ… **Outlier Detection**: Statistical anomaly identification  
âœ… **Quality Scoring**: Overall data health metrics  

### Analytics & KPIs
âœ… **Financial Metrics**: Revenue, profit, ROI calculations  
âœ… **Performance Rankings**: Top/bottom movies by various metrics  
âœ… **Temporal Analysis**: Yearly trends and patterns  
âœ… **Genre Analysis**: Genre-specific performance metrics  
âœ… **Franchise Comparison**: Franchise vs standalone analysis  
âœ… **Director Analytics**: Director performance metrics  

### Visualization
âœ… **Interactive Dashboards**: Web-based reporting  
âœ… **Trend Visualizations**: Time-series analysis  
âœ… **Correlation Plots**: Multi-dimensional analysis  
âœ… **Distribution Charts**: Statistical distributions  

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              User Interface Layer               â”‚
â”‚   JupyterLab â”‚ Airflow UI â”‚ Grafana â”‚ Reports  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Orchestration Layer                   â”‚
â”‚           Apache Airflow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Processing Layer                      â”‚
â”‚   Spark Master â†â†’ Worker-1 â†â†’ Worker-2         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Data Layer                         â”‚
â”‚   PostgreSQL â”‚ Redis Cache â”‚ File Storage      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
tmdb-movie-analysis/
â”œâ”€â”€ docker/                      # Docker configurations
â”‚   â”œâ”€â”€ docker-compose.yml      # Multi-service orchestration
â”‚   â”œâ”€â”€ Dockerfile.spark        # Spark image
â”‚   â”œâ”€â”€ Dockerfile.airflow      # Airflow image
â”‚   â””â”€â”€ Dockerfile.jupyter      # Jupyter image
â”‚
â”œâ”€â”€ airflow/                     # Airflow components
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ tmdb_pipeline_dag.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ config/
â”‚
â”œâ”€â”€ src/                         # Source code
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ config.yaml         # Central configuration
â”‚   â”‚   â””â”€â”€ logging_config.py   # Logging setup
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/              # Data fetching
â”‚   â”‚   â”œâ”€â”€ api_client.py       # TMDB API client
â”‚   â”‚   â””â”€â”€ data_fetcher.py     # Data fetching logic
â”‚   â”‚
â”‚   â”œâ”€â”€ processing/             # Data processing
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py     # Data cleaning
â”‚   â”‚   â”œâ”€â”€ data_transformer.py # Feature engineering
â”‚   â”‚   â””â”€â”€ data_validator.py   # Quality validation
â”‚   â”‚
â”‚   â”œâ”€â”€ analytics/              # Analytics & KPIs
â”‚   â”‚   â”œâ”€â”€ kpi_calculator.py   # KPI calculations
â”‚   â”‚   â”œâ”€â”€ advanced_queries.py # Complex queries
â”‚   â”‚   â””â”€â”€ metrics_aggregator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/          # Visualizations
â”‚   â”‚   â””â”€â”€ dashboard_generator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                  # Utilities
â”‚   â”‚   â”œâ”€â”€ spark_session.py    # Spark management
â”‚   â”‚   â””â”€â”€ helpers.py          # Helper functions
â”‚   â”‚
â”‚   â””â”€â”€ main.py                 # Main pipeline
â”‚
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”‚
â”œâ”€â”€ data/                        # Data directories
â”‚   â”œâ”€â”€ raw/                    # Raw API data
â”‚   â”œâ”€â”€ processed/              # Cleaned data
â”‚   â””â”€â”€ output/                 # Results & reports
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ api_documentation.md
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Makefile                    # Automation commands
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Docker Desktop (24.0+)
- Docker Compose v2
- 8GB+ RAM recommended
- TMDB API Key ([Get it here](https://www.themoviedb.org/settings/api))

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd tmdb-movie-analysis
```

2. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your TMDB API key and configurations
nano .env
```

3. **Start all services**
```bash
make setup
make up
```

4. **Access the applications**
- **Airflow**: http://localhost:8081 (admin/admin)
- **Spark Master UI**: http://localhost:8080
- **JupyterLab**: http://localhost:8888
- **Grafana**: http://localhost:3000 (admin/admin)

5. **Run the pipeline**
```bash
# Via Airflow UI (recommended)
# Navigate to http://localhost:8081 and trigger the DAG

# Or via command line
make run-pipeline
```

## ğŸ“‹ Configuration

### Environment Variables (.env)
```bash
# API Configuration
TMDB_API_KEY=your_tmdb_api_key

# Database
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow

# Redis
REDIS_PASSWORD=redis_secret

# Airflow
AIRFLOW_UID=50000
AIRFLOW__CORE__FERNET_KEY=your_fernet_key
```

### Pipeline Configuration (config.yaml)
```yaml
api:
  base_url: "https://api.themoviedb.org/3"
  rate_limit:
    requests_per_second: 40

spark:
  driver_memory: "4g"
  executor_memory: "4g"
  shuffle_partitions: "200"

pipeline:
  batch_size: 100
  checkpoint_interval: 10
```

## ğŸ¯ Pipeline Stages

### 1. Data Ingestion
- Fetches movie data from TMDB API
- Implements rate limiting and caching
- Handles API failures with retry logic

### 2. Data Cleaning
- Removes irrelevant columns
- Handles missing values
- Fixes data type issues
- Removes duplicates
- Standardizes formats

### 3. Data Transformation
- Feature engineering
- Calculated fields (profit, ROI)
- Date parsing and extraction
- Multi-value field processing

### 4. Data Validation
- Schema validation
- Business rule checks
- Data quality scoring
- Outlier detection

### 5. Analytics & KPIs
- Financial metrics (revenue, profit, ROI)
- Performance rankings
- Genre analysis
- Director/franchise analytics
- Temporal trends

### 6. Visualization
- Revenue vs budget plots
- Genre distributions
- Yearly trends
- ROI distributions
- Rating correlations
- Franchise comparisons

## ğŸ“Š Sample KPIs

### Financial Metrics
- **Highest Revenue Movies**
- **Highest Profit Movies**
- **Best ROI (Budget â‰¥ $10M)**
- **Worst ROI**

### Quality Metrics
- **Highest Rated Movies** (min 10 votes)
- **Most Popular Movies**
- **Most Voted Movies**

### Analysis Queries
- Franchise vs Standalone comparison
- Genre-specific performance
- Director performance metrics
- Yearly box office trends

## ğŸ› ï¸ Development

### Running Tests
```bash
# Unit tests
make test

# Integration tests
make test-integration

# Coverage report
make test-coverage
```

### Code Quality
```bash
# Format code
make format

# Lint code
make lint

# Type checking
make type-check
```

### Debugging
```bash
# View logs
make logs

# Enter Spark container
make spark-shell

# Check service health
make health-check
```

## ğŸ“ˆ Monitoring

### Metrics Available
- Pipeline execution time
- Data quality scores
- API call statistics
- Cache hit rates
- Spark job metrics

### Dashboards
- **Grafana**: Real-time metrics (http://localhost:3000)
- **Spark UI**: Job execution details (http://localhost:8080)
- **Airflow UI**: Workflow status (http://localhost:8081)

## ğŸ§ª Testing Strategy

### Unit Tests
- Individual component testing
- Mock external dependencies
- Fast execution

### Integration Tests
- End-to-end pipeline testing
- Real Spark sessions
- Database interactions

### Data Quality Tests
- Schema validation tests
- Business rule tests
- Edge case handling

## ğŸ”§ Troubleshooting

### Common Issues

**Spark Out of Memory**
```yaml
# Increase memory in docker-compose.yml
spark-master:
  environment:
    - SPARK_DRIVER_MEMORY=8g
```

**API Rate Limiting**
```yaml
# Reduce rate in config.yaml
api:
  rate_limit:
    requests_per_second: 20
```

**Database Connection Issues**
```bash
# Reset database
docker-compose down -v
docker-compose up -d
```

## ğŸ“š Documentation

- [Complete Implementation Guide](COMPLETE_IMPLEMENTATION_GUIDE.md)
- [Architecture Documentation](docs/architecture.md)
- [API Documentation](docs/api_documentation.md)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¥ Authors

- Your Name - *Initial work*

## ğŸ™ Acknowledgments

- TMDB for providing the API
- Apache Spark community
- Apache Airflow community

## ğŸ“ Support

- Documentation: See `docs/` directory
- Issues: GitHub Issues
- Email: your-email@example.com

---

**Happy Data Engineering! ğŸš€**